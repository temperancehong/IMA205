{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Data preprocessing functions and helper functions inspired by the PyTorch tutorial [Oxford IIIT Pets Segmentation using PyTorch](https://github.com/dhruvbird/ml-notebooks/blob/main/pets_segmentation/oxford-iiit-pets-segmentation-using-pytorch-segnet-and-depth-wise-separable-convs.ipynb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from os import path\n",
    "import torchvision\n",
    "import torchvision.transforms as T # for later data augmentation\n",
    "from typing import Sequence\n",
    "from torch.nn import functional as F\n",
    "import numbers\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import torchmetrics as TM\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset,  DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for better visualization during preprocessing, we view tensors and PIL images\n",
    "t2img = T.ToPILImage() # tensor to pil image\n",
    "img2t = T.ToTensor() # pil image to tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Cuda: {torch.cuda.is_available()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "working_dir = './'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Useful tool functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save model status and checkpoint\n",
    "def save_model_checkpoint(model, cp_name):\n",
    "    torch.save(model.state_dict(), os.path.join(working_dir, cp_name))\n",
    "\n",
    "# get the current device: cpu or gpu\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# Load model from saved checkpoint\n",
    "# If map_location is a torch.device object or a string containing a device tag, it indicates the location where all tensors should be loaded.\n",
    "def load_model_from_checkpoint(model, ckp_path):\n",
    "    return model.load_state_dict(\n",
    "        torch.load(\n",
    "            ckp_path,\n",
    "            map_location=get_device(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# send tensor and model to the device\n",
    "def to_device(x):\n",
    "    if torch.cuda.is_available():\n",
    "        return x.cuda()\n",
    "    else:\n",
    "        return x.cpu()\n",
    "\n",
    "# get the total number of parameters of the model\n",
    "def get_model_parameters(m): # numel: total number of elements\n",
    "    total_params = sum(\n",
    "        param.numel() for param in m.parameters()\n",
    "    )\n",
    "    return total_params\n",
    "\n",
    "# print the total number of parameters\n",
    "def print_model_parameters(m):\n",
    "    num_model_parameters = get_model_parameters(m)\n",
    "    print(f\"The Model has {num_model_parameters/1e6:.2f}M parameters\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Simple torchvision compatible transform to send an input tensor\n",
    "# to a pre-specified device.\n",
    "class ToDevice(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Sends the input object to the device specified in the\n",
    "    object's constructor by calling .to(device) on the object.\n",
    "    \"\"\"\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, img):\n",
    "        return img.to(self.device)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(device={device})\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Explore the data\n",
    "\n",
    "The images are jpg images, with three channels and values between 0 and 255."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_image = Image.open('./data/Train/Train/ISIC_0000000.jpg')\n",
    "plt.imshow(sample_image)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_image_array = np.array(sample_image)\n",
    "sample_image_array.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_image_gt = Image.open('./data/Train/Train/ISIC_0000000_seg.png')\n",
    "sample_image_gt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_image_gt_array = np.array(sample_image_gt)\n",
    "sample_image_gt_array.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that for the ground truth, we have a single channel image with values between 0 and 255."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.unique(sample_image_gt_array)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we concate the ground truth mask as the fourth channel of the image\n",
    "sample_image_array = np.concatenate((sample_image_array, np.expand_dims(sample_image_gt_array, axis=2)), axis=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_image_array"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing\n",
    "\n",
    "We concate the ground truth mask as the fourth channel of the image for all the image in the dataset.\n",
    "\n",
    "In the meantime, we should notice that not all the images have a ground truth mask. We should only concate the ground truth mask for the images that have it. If they represent for the majority image, we can drop the images without ground truth mask. If not, we can use a different approach to handle the missing ground truth mask."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a list for all the concatenated images\n",
    "all_images = []\n",
    "# create a list for the name of the images which have missing ground truth mask\n",
    "missing_gt = []\n",
    "# iterate through the directory, we can get the name of the image because they are in jpg format, and then get the corresponding ground truth mask by adding _seg.png at the end and they are in png format and in the same directory;\n",
    "# also, we print a message if the ground truth mask is missing\n",
    "for image in os.listdir('./data/Train/Train'):\n",
    "    if image.endswith('.jpg'):\n",
    "        image_name = image.split('.')[0]\n",
    "        image_path = f'./data/Train/Train/{image}'\n",
    "        image = Image.open(image_path)\n",
    "        image_array = np.array(image)\n",
    "        gt_path = f'./data/Train/Train/{image_name}_seg.png'\n",
    "        if path.exists(gt_path):\n",
    "            gt = Image.open(gt_path)\n",
    "            gt_array = np.array(gt)\n",
    "            image_array = np.concatenate((image_array, np.expand_dims(gt_array, axis=2)), axis=2)\n",
    "            all_images.append(image_array)\n",
    "        else:\n",
    "            print(f\"Ground truth mask is missing for {image_name}\")\n",
    "            missing_gt.append(image_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(missing_gt)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(all_images)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the benchmark model, we use all the images, we ignore the existence of the ground truth masks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 1: benchmark with nearly no preprocessing)\n",
    "\n",
    "We observe that as input, we can put sex, age and pathology position as well as features, but here we ignore it in the bench mark model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Creation\n",
    "\n",
    "We take all the jpg pictures, and put them in a huge array to be the dataset.\n",
    "\n",
    "We then do a train-test split, and create a dataloader for each of them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# read the images according to the order of the dataframe image name\n",
    "\n",
    "# read the metadata\n",
    "meta_data_train = pd.read_csv('./data/metadataTrain.csv')\n",
    "y = meta_data_train['CLASS'].values\n",
    "y = y-1\n",
    "\n",
    "# get the image names\n",
    "image_names = list(meta_data_train['ID'].values)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "all_images = []\n",
    "for image in image_names:\n",
    "    image_path = f'./data/Train/Train/{image}.jpg'\n",
    "    image = Image.open(image_path)\n",
    "    image_array = np.array(image)\n",
    "    all_images.append(image_array)\n",
    "    # print(image_array.shape)\n",
    "print(len(all_images))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # create a list for all the jpg images\n",
    "# all_images = []\n",
    "# # iterate through the directory, read them in as 3 channel images, and then append them to the list\n",
    "# for image in os.listdir('./data/Train/Train'):\n",
    "#     if image.endswith('.jpg'):\n",
    "#         image_path = f'./data/Train/Train/{image}'\n",
    "#         image = Image.open(image_path)\n",
    "#         image_array = np.array(image)\n",
    "#         all_images.append(image_array)\n",
    "#         # print(image_array.shape)\n",
    "# print(len(all_images))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then realize that the images have different sizes, so we need to resize them to the same size."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we will make all the images into the size of 512*512, which is the largest size, for those who are smaller, we use zero padding, there is not larger images\n",
    "# write a loop to resize all the image arrays\n",
    "all_images_pad = []\n",
    "for i in range(len(all_images)):\n",
    "    h, w, _ = all_images[i].shape\n",
    "    # Initialize a new array of zeros of shape (512, 512, 3) for padding\n",
    "    padded_image = np.zeros((512, 512, 3), dtype=all_images[0].dtype)\n",
    "\n",
    "    # Calculate padding sizes\n",
    "    pad_h = (512 - h) // 2\n",
    "    pad_w = (512 - w) // 2\n",
    "\n",
    "    # Update the padded_image with the original image centered\n",
    "    padded_image[pad_h:pad_h+h, pad_w:pad_w+w, :] = all_images[i]\n",
    "\n",
    "    # Append the padded image to the list\n",
    "    all_images_pad.append(padded_image)\n",
    "    if i % 5000 == 0:\n",
    "        print(f\"{i} images have been padded\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# put the list into an array\n",
    "# all_images_pad = np.array(all_images_pad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# reduce the resolution of the images to 256*256\n",
    "all_images_pad = np.array([np.array(Image.fromarray(image).resize((128, 128))) for image in all_images_pad])"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we create the feature tensor, and also read in the csv to get the target labels."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(all_images_pad, y, test_size=0.2, random_state=42, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_arrays, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_arrays (np.array): An array of Numpy arrays representing the images.\n",
    "            labels (np.array): An array of labels.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.image_arrays = image_arrays\n",
    "        # Ensure labels are in a compatible format (np.array or list)\n",
    "        self.labels = np.array(labels) if not isinstance(labels, np.ndarray) else labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_arrays)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.image_arrays[idx]\n",
    "\n",
    "        # Convert image from numpy array to PIL Image to apply torchvision transforms\n",
    "        image = Image.fromarray(image.astype('uint8'), 'RGB')  # Ensure the data type and mode are correct\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "# Make sure X_train, y_train, X_test, y_test are defined and correctly formatted\n",
    "# For example purposes, assuming they're already defined and loaded...\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset_train = CustomImageDataset(X_train, y_train, transform=transform)\n",
    "dataset_test = CustomImageDataset(X_test, y_test, transform=transform)\n",
    "\n",
    "batch_size = 32  # Define your batch size\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(train_inputs_0, train_targets_0) = next(iter(dataloader_train))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "(test_inputs, test_targets) = next(iter(dataloader_test))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_targets_0.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_inputs_0.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "empty_tensor = torch.empty(128, 128, 3)\n",
    "sample_tensor0 = torch.tensor(all_images_pad[0]/255, dtype=torch.float32)\n",
    "sample_tensor1 = torch.tensor(all_images_pad[1]/255, dtype=torch.float32)\n",
    "torch.stack((empty_tensor, sample_tensor0), dim=0).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build the Beginner Model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BeginnerModel(nn.Module):\n",
    "    def __init__(self, n_classes=8, in_pixel=128):\n",
    "        super(BeginnerModel, self).__init__()\n",
    "        self.in_pixel = in_pixel\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1) # RGB channels as input\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * (self.in_pixel//8) * (self.in_pixel//8), n_classes)  # in_pixel//8 because of 3 maxpool layers\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        x = x.view(-1, 128 * (self.in_pixel//8) * (self.in_pixel//8))\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # with inference mode\n",
    "# class BeginnerModel(nn.Module):\n",
    "#     def __init__(self, n_classes=8, in_pixel=128):\n",
    "#         super(BeginnerModel, self).__init__()\n",
    "#         self.in_pixel = in_pixel\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "#         self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "#         self.fc1 = nn.Linear(128 * (in_pixel//8) * (in_pixel//8), n_classes)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         # self.inference_mode = False  # Add an attribute to toggle inference mode\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = self.pool(F.relu(self.conv3(x)))\n",
    "#         x = x.view(-1, 128 * (self.in_pixel//8) * (self.in_pixel//8))\n",
    "#         x = self.fc1(x)\n",
    "#         if self.inference_mode:\n",
    "#             x = torch.argmax(x, dim=1)  # Convert logits to class indices during inference\n",
    "#         return x\n",
    "# \n",
    "#     def enable_inference_mode(self, enable=True):\n",
    "#         self.inference_mode = enable\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define the model\n",
    "m = BeginnerModel(n_classes=8)\n",
    "m.eval()\n",
    "to_device(m)\n",
    "m(to_device(train_inputs_0)).shape # input the first batch to test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## simple training function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "to_device(m)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=0.001)  # Optimizer\n",
    "\n",
    "num_epochs = 10  # Number of epochs to train for\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "# Train the model\n",
    "train_model(m, dataloader_train, criterion, optimizer, num_epochs, device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# get the precision of the prediction from the test dataloader\n",
    "def test_model(model, test_loader, criterion, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    loss = running_loss / len(test_loader.dataset)\n",
    "    accuracy = running_corrects / len(test_loader.dataset)\n",
    "    print(f'Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_model(m, dataloader_test, criterion, device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use the model to predict submission dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "meta_data_submission = pd.read_csv('./data/metadataTest.csv')\n",
    "id_list = list(meta_data_submission['ID'].values)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "all_images_submission = []\n",
    "for image in id_list:\n",
    "    image_path = f'./data/Test/Test/{image}.jpg'\n",
    "    image = Image.open(image_path)\n",
    "    image_array = np.array(image)\n",
    "    all_images_submission.append(image_array)\n",
    "    # print(image_array.shape)\n",
    "print(len(all_images_submission))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# we will make all the images into the size of 512*512, which is the largest size, for those who are smaller, we use zero padding, there is not larger images\n",
    "# write a loop to resize all the image arrays\n",
    "all_images_submission_pad = []\n",
    "for i in range(len(all_images_submission)):\n",
    "    h, w, _ = all_images_submission[i].shape\n",
    "    # Initialize a new array of zeros of shape (512, 512, 3) for padding\n",
    "    padded_image = np.zeros((512, 512, 3), dtype=all_images_submission[0].dtype)\n",
    "\n",
    "    # Calculate padding sizes\n",
    "    pad_h = (512 - h) // 2\n",
    "    pad_w = (512 - w) // 2\n",
    "\n",
    "    # Update the padded_image with the original image centered\n",
    "    padded_image[pad_h:pad_h+h, pad_w:pad_w+w, :] = all_images_submission[i]\n",
    "\n",
    "    # Append the padded image to the list\n",
    "    all_images_submission_pad.append(padded_image)\n",
    "    if i % 5000 == 0:\n",
    "        print(f\"{i} images have been padded\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# reduce the resolution of the images to 128*128\n",
    "all_images_submission_pad = np.array([np.array(Image.fromarray(image).resize((128, 128))) for image in all_images_submission_pad])"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "all_images_submission_pad.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "ssh project 17\n",
    "\n",
    "scp fhong-22@gpu5.enst.fr:/tmp/pycharm_project_17/classification_challenge/submission_benchmark.csv E:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Assuming `data_array` is your numpy array of shape (6333, 128, 128, 3)\n",
    "# and `model` is your trained PyTorch model\n",
    "\n",
    "# Step 1: Convert the numpy array to a PyTorch tensor and transpose it to (Batch, Channels, Height, Width)\n",
    "data_tensor = torch.tensor(all_images_submission_pad).float()\n",
    "data_tensor = data_tensor.permute(0, 3, 1, 2)\n",
    "\n",
    "# Step 2: Create a DataLoader\n",
    "batch_size = 16  # You can adjust the batch size\n",
    "dataset_submission = TensorDataset(data_tensor)\n",
    "data_loader_submission = DataLoader(dataset_submission, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Step 3: Set the model to evaluation mode\n",
    "m.eval()\n",
    "\n",
    "# Prepare device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "to_device(m)\n",
    "\n",
    "# Step 4: Generate Predictions\n",
    "predictions = []\n",
    "with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "    for batch in data_loader_submission:\n",
    "        inputs = to_device(batch[0])\n",
    "        # inputs = batch[0].to(device)\n",
    "        outputs = m(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
    "        predictions.extend(predicted.cpu().numpy())"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "predictions = [prediction+1 for prediction in predictions]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame({'ID': id_list, 'CLASS': predictions})\n",
    "df_submission.to_csv('submission_benchmark.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# draft"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "ima-env-ssh",
   "language": "python",
   "display_name": "ima-env-ssh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
